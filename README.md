# PaliGemma Vision Language Model from scratch

This repository will contain the implementation of a **Multimodal (Vision) Language Model** developed from scratch using Python and PyTorch. The model is inspired by the PaliGemma architecture and covers multiple key concepts in natural language processing and computer vision

## Project Overview

This project implements a Vision Language Model, including both **Transformer** and **Vision Transformer** architectures. We explore various techniques and concepts such as **Contrastive Learning** (CLIP, SigLip), **Rotary Positional Embedding**, **Attention Mechanisms**, and more.

### Key Concepts and Techniques

1. **Transformer Architecture**
   - Embeddings
   - Positional Encoding
   - Multi-Head Attention
   - Feed Forward Layer
   - Logits and Softmax

2. **Vision Transformer Model**

3. **Contrastive Learning**
   - CLIP (Contrastive Languageâ€“Image Pretraining)
   - SigLip (Sigmoid Contrastive Learning)
   
4. **Numerical Stability**
   - Softmax Stability
   - Cross Entropy Loss

5. **Advanced Embeddings**
   - Rotary Positional Embedding

6. **Attention Mechanisms**
   - Multi-Head Attention
   - Grouped Query Attention
   - KV-Cache (Prefilling and Token Generation)
   - Causal and Non-Causal Attention Masks

7. **Normalization Layers**
   - Batch Normalization
   - Layer Normalization
   - RMS Normalization

8. **Sampling Techniques**
   - Weight Tying
   - Top-P Sampling
   - Temperature Scaling
